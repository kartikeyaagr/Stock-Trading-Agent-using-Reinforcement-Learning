{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rainbow DQN Implementation with NoisyNets for Real Time Stock Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3RQHfNpNYfEO"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eXFdsrE3YfEP"
      },
      "outputs": [],
      "source": [
        "def collect_stock_data():\n",
        "    print(\"Loading pre-downloaded stock data...\")\n",
        "    DATA_FILE_PATH = \"data/train_data.csv\"\n",
        "    try:\n",
        "        stock_data = pd.read_csv(DATA_FILE_PATH, index_col=\"Date\", parse_dates=True)\n",
        "        print(\"Data loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Data file not found at {DATA_FILE_PATH}\")\n",
        "        print(\"Please run the data download script first.\")\n",
        "        exit()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data from file: {e}\")\n",
        "        exit()\n",
        "    return stock_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "71VdNy58YfEQ"
      },
      "outputs": [],
      "source": [
        "def calculate_technical_indicators(df):\n",
        "    # Calculate returns\n",
        "    df[\"Returns\"] = df[\"Close\"].pct_change()\n",
        "\n",
        "    # Calculate moving averages\n",
        "    df[\"SMA_20\"] = df[\"Close\"].rolling(window=20).mean()\n",
        "    df[\"SMA_50\"] = df[\"Close\"].rolling(window=50).mean()\n",
        "\n",
        "    # Calculate RSI\n",
        "    delta = df[\"Close\"].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # Calculate MACD\n",
        "    exp1 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
        "    exp2 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
        "    df[\"MACD\"] = exp1 - exp2\n",
        "    df[\"Signal_Line\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    # Calculate Bollinger Bands\n",
        "    rolling_mean = df[\"Close\"].rolling(window=20).mean()\n",
        "    rolling_std = df[\"Close\"].rolling(window=20).std()\n",
        "    df[\"BB_middle\"] = rolling_mean\n",
        "    df[\"BB_upper\"] = rolling_mean + (2 * rolling_std)\n",
        "    df[\"BB_lower\"] = rolling_mean - (2 * rolling_std)\n",
        "\n",
        "    # Calculate volatility\n",
        "    df[\"Volatility\"] = df[\"Returns\"].rolling(window=20).std()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VvXJYlHFYfEQ"
      },
      "outputs": [],
      "source": [
        "class TradingEnvironment:\n",
        "    def __init__(self, data: pd.DataFrame, initial_balance=100000):\n",
        "\n",
        "        if data.isnull().values.any():\n",
        "\n",
        "            nan_rows = data[data.isnull().any(axis=1)]\n",
        "            print(\n",
        "                \"Warning: DataFrame passed to TradingEnvironment contains NaN values.\"\n",
        "            )\n",
        "            print(\"Ensure .dropna() was called after calculating indicators.\")\n",
        "            print(\"First few rows with NaNs:\\n\", nan_rows.head())\n",
        "\n",
        "            raise ValueError(\n",
        "                \"DataFrame contains NaNs. Please clean before passing to Environment.\"\n",
        "            )\n",
        "\n",
        "        self.data = data.copy()\n",
        "        self.initial_balance = initial_balance\n",
        "\n",
        "        self.feature_columns = [\n",
        "            \"Returns\",\n",
        "            \"SMA_20\",\n",
        "            \"SMA_50\",\n",
        "            \"RSI\",\n",
        "            \"MACD\",\n",
        "            \"Signal_Line\",\n",
        "            \"BB_upper\",\n",
        "            \"BB_lower\",\n",
        "            \"Volatility\",\n",
        "        ]\n",
        "        self.price_column = \"Close\"\n",
        "\n",
        "        # --- Verify all required columns are present ---\n",
        "        required_cols_for_state = self.feature_columns + [self.price_column]\n",
        "        missing_cols = [\n",
        "            col for col in required_cols_for_state if col not in self.data.columns\n",
        "        ]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns in input data: {missing_cols}\")\n",
        "\n",
        "        self.state_size = 3 + len(self.feature_columns)\n",
        "        print(f\"Environment initialized. State size: {self.state_size}\")\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
        "        self.balance = self.initial_balance\n",
        "        self.current_step = 0  # Start from the first row (assuming NaNs are dropped)\n",
        "        self.position = 0  # Number of shares held\n",
        "        self.portfolio_value = self.initial_balance\n",
        "        self.returns_history = []\n",
        "        # print(f\"Environment reset. Starting step: {self.current_step}, Index: {self.data.index[self.current_step]}\")\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "\n",
        "        if self.current_step >= len(self.data):\n",
        "            # Should not happen if step logic is correct, but safeguard\n",
        "            print(\n",
        "                f\"Warning: _get_state called at step {self.current_step} >= data length {len(self.data)}. Using last valid data.\"\n",
        "            )\n",
        "            self.current_step = len(self.data) - 1\n",
        "\n",
        "        current_data = self.data.iloc[self.current_step]\n",
        "        current_price = current_data[self.price_column]\n",
        "\n",
        "        # --- 1. Agent Status Features (Normalized) ---\n",
        "        position_value = self.position * current_price\n",
        "        normalized_position_value = (\n",
        "            position_value / self.portfolio_value\n",
        "            if self.portfolio_value > 1e-6\n",
        "            else 0.0\n",
        "        )  # Avoid div by zero\n",
        "        normalized_balance = self.balance / self.initial_balance\n",
        "        normalized_portfolio_value = self.portfolio_value / self.initial_balance\n",
        "\n",
        "        agent_state = [\n",
        "            normalized_position_value,\n",
        "            normalized_balance,\n",
        "            normalized_portfolio_value,\n",
        "        ]\n",
        "\n",
        "        # --- 2. Technical Indicator Features (Scaled/Normalized) ---\n",
        "        tech_state = []\n",
        "        # Use a small epsilon to prevent division by zero if price is exactly zero\n",
        "        epsilon = 1e-8\n",
        "        safe_price = current_price if abs(current_price) > epsilon else epsilon\n",
        "\n",
        "        # Iterate through defined feature columns and apply scaling/normalization\n",
        "        for col in self.feature_columns:\n",
        "            value = current_data[col]\n",
        "            if pd.isna(value):\n",
        "                # This shouldn't happen if data is pre-cleaned, but as a fallback\n",
        "                print(\n",
        "                    f\"Warning: NaN found in column '{col}' at step {self.current_step}. Replacing with 0.\"\n",
        "                )\n",
        "                value = 0.0\n",
        "\n",
        "            # Apply scaling/normalization based on the indicator type\n",
        "            if col == \"Returns\":\n",
        "                # Returns are often small, maybe scale slightly? Or keep as is. Let's keep as is for now.\n",
        "                scaled_value = value\n",
        "            elif col in [\n",
        "                \"SMA_20\",\n",
        "                \"SMA_50\",\n",
        "                \"BB_upper\",\n",
        "                \"BB_lower\",\n",
        "                \"MACD\",\n",
        "                \"Signal_Line\",\n",
        "            ]:\n",
        "                # Normalize price-based indicators relative to the current price\n",
        "                scaled_value = (value - current_price) / safe_price\n",
        "            elif col == \"RSI\":\n",
        "                # Scale RSI from [0, 100] to [0.0, 1.0]\n",
        "                scaled_value = value / 100.0\n",
        "            elif col == \"Volatility\":\n",
        "                # Volatility is a std dev (percentage), usually small. Keep as is for now.\n",
        "                scaled_value = value\n",
        "            else:\n",
        "                # Default for any unexpected columns (shouldn't happen with check in init)\n",
        "                scaled_value = value\n",
        "\n",
        "            tech_state.append(scaled_value)\n",
        "\n",
        "        # --- Combine states ---\n",
        "        state = agent_state + tech_state\n",
        "        state_np = np.array(state, dtype=np.float32)\n",
        "        if np.any(np.isnan(state_np)) or np.any(np.isinf(state_np)):\n",
        "\n",
        "            state_np = np.nan_to_num(\n",
        "                state_np, nan=0.0, posinf=0.0, neginf=0.0\n",
        "            )\n",
        "\n",
        "        if len(state_np) != self.state_size:\n",
        "            raise RuntimeError(\n",
        "                f\"Internal Error: State size mismatch. Expected {self.state_size}, got {len(state_np)}\"\n",
        "            )\n",
        "\n",
        "        return state_np\n",
        "\n",
        "    def step(self, action):\n",
        "        # Actions: 0 = hold, 1 = buy, 2 = sell\n",
        "\n",
        "        if self.current_step >= len(self.data) - 2:  # Need current and next price\n",
        "            # print(f\"Attempting to step beyond data bounds (step {self.current_step}). Ending episode.\")\n",
        "            current_state = self._get_state()  # Get the last valid state\n",
        "            return current_state, 0.0, True \n",
        "\n",
        "        current_data = self.data.iloc[self.current_step]\n",
        "        next_data = self.data.iloc[self.current_step + 1]\n",
        "\n",
        "        price = current_data[self.price_column]\n",
        "        next_price = next_data[self.price_column]\n",
        "\n",
        "        # Handle potential NaN prices more robustly\n",
        "        if pd.isna(price):\n",
        "            # Price is NaN: Cannot trade reliably. Treat as forced hold, zero reward.\n",
        "            # print(f\"Warning: NaN price encountered at step {self.current_step}. Forcing hold.\")\n",
        "            reward = 0.0\n",
        "            done = self.current_step >= len(self.data) - 2  # Check done condition again\n",
        "            self.current_step += 1  # Move step forward\n",
        "            # Portfolio value likely shouldn't change if price is NaN\n",
        "            # self.portfolio_value remains the same\n",
        "            return self._get_state(), reward, done\n",
        "        if pd.isna(next_price):\n",
        "            # Next price is NaN: Can execute trade at current price, but reward/next value is uncertain.\n",
        "            # Option: use current price for next value calculation (conservative)\n",
        "            # print(f\"Warning: NaN next_price encountered at step {self.current_step+1}. Using current price for value calculation.\")\n",
        "            next_price = price  # Fallback\n",
        "\n",
        "        initial_portfolio_value = self.portfolio_value\n",
        "\n",
        "        if action == 1:  # Buy\n",
        "            if (\n",
        "                self.position == 0 and self.balance > price\n",
        "            ):  # Check if enough balance for at least 1 share\n",
        "                shares_to_buy = self.balance // price\n",
        "                if shares_to_buy > 0:\n",
        "                    cost = shares_to_buy * price\n",
        "                    self.balance -= cost\n",
        "                    self.position = shares_to_buy\n",
        "            # If already holding or not enough balance, treat as hold\n",
        "\n",
        "        elif action == 2:  # Sell\n",
        "            if self.position > 0:\n",
        "                revenue = self.position * price\n",
        "                self.balance += revenue\n",
        "                self.position = 0\n",
        "            # If not holding, treat as hold\n",
        "\n",
        "        # Update portfolio value using next_price (potentially the fallback price)\n",
        "        portfolio_value = self.balance + self.position * next_price\n",
        "\n",
        "        # Calculate returns based on the change from initial value for this step\n",
        "        if initial_portfolio_value > 1e-6:  # Avoid division by zero\n",
        "            returns = (\n",
        "                portfolio_value - initial_portfolio_value\n",
        "            ) / initial_portfolio_value\n",
        "        else:\n",
        "            returns = 0.0\n",
        "\n",
        "        self.returns_history.append(returns)\n",
        "        self.portfolio_value = (\n",
        "            portfolio_value  # Update portfolio value tracked by the env\n",
        "        )\n",
        "\n",
        "        # Advance step *before* getting the next state\n",
        "        self.current_step += 1\n",
        "\n",
        "        # CVaR reward adjustment\n",
        "        reward = returns\n",
        "        alpha = 0.05\n",
        "        min_history_for_cvar = 20\n",
        "        if len(self.returns_history) >= min_history_for_cvar:\n",
        "            cvar_penalty_factor = 0.1\n",
        "            calculated_cvar = self._calculate_cvar(\n",
        "                self.returns_history[-min_history_for_cvar:], alpha=alpha\n",
        "            )\n",
        "            reward = returns - cvar_penalty_factor * abs(calculated_cvar)\n",
        "\n",
        "        # Check if done (reached the end of data)\n",
        "        # Now done condition is simpler: if current_step points beyond the last valid index\n",
        "        done = (\n",
        "            self.current_step >= len(self.data) - 1\n",
        "        )  # -1 because step was already incremented\n",
        "\n",
        "        next_state = self._get_state()  # Get state for the *new* current_step\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def _calculate_cvar(self, returns, alpha=0.05):\n",
        "        if not isinstance(returns, np.ndarray):\n",
        "            returns = np.array(returns)\n",
        "        if len(returns) == 0:\n",
        "            return 0.0\n",
        "        var = np.percentile(returns, alpha * 100)\n",
        "        cvar = returns[returns <= var].mean()\n",
        "        return cvar if not np.isnan(cvar) else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PK_J0UPoYfER"
      },
      "outputs": [],
      "source": [
        "# Assuming NoisyLinear class is defined as before:\n",
        "class NoisyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std_init=0.5):\n",
        "        super(NoisyLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.std_init = std_init\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
        "        self.register_buffer(\"weight_epsilon\", torch.empty(out_features, in_features))\n",
        "\n",
        "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
        "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
        "        self.register_buffer(\"bias_epsilon\", torch.empty(out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "        self.reset_noise()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        mu_range = 1 / np.sqrt(self.in_features)\n",
        "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
        "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
        "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
        "\n",
        "    def reset_noise(self):\n",
        "        epsilon_in = self._scale_noise(self.in_features)\n",
        "        epsilon_out = self._scale_noise(self.out_features)\n",
        "        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n",
        "        self.bias_epsilon.copy_(epsilon_out)\n",
        "\n",
        "    def _scale_noise(self, size):\n",
        "        x = torch.randn(\n",
        "            size, device=self.weight_mu.device\n",
        "        )  # Ensure noise is on correct device\n",
        "        return x.sign().mul_(x.abs().sqrt_())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure noise tensors are on the same device as parameters/input\n",
        "        if self.weight_epsilon.device != x.device:\n",
        "            self.weight_epsilon = self.weight_epsilon.to(x.device)\n",
        "            self.bias_epsilon = self.bias_epsilon.to(x.device)\n",
        "            # print(f\"Moved noise to {x.device} in NoisyLinear\") # Optional debug print\n",
        "\n",
        "        if self.training:\n",
        "            # Sample new noise only if training\n",
        "            self.reset_noise()\n",
        "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
        "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
        "        else:\n",
        "            # Use mean weights/biases during evaluation\n",
        "            weight = self.weight_mu\n",
        "            bias = self.bias_mu\n",
        "        return nn.functional.linear(x, weight, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YDrWD-r0YfES"
      },
      "outputs": [],
      "source": [
        "class RainbowDQN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_size,\n",
        "        action_size,\n",
        "        num_atoms=51,\n",
        "        v_min=-10,\n",
        "        v_max=10,\n",
        "        hidden_size=128,\n",
        "        device=None,\n",
        "    ):\n",
        "        super(RainbowDQN, self).__init__()\n",
        "\n",
        "        # device setup\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Initializing RainbowDQN on device: {self.device}\")\n",
        "\n",
        "        # save sizes\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.num_atoms = num_atoms\n",
        "        self.v_min = v_min\n",
        "        self.v_max = v_max\n",
        "\n",
        "        # support for C51 (distributional)\n",
        "        self.support = torch.linspace(v_min, v_max, num_atoms).to(self.device)\n",
        "\n",
        "        # --- Shared Feature Extraction Layers ---\n",
        "        # Using NoisyLinear for exploration baked into the network\n",
        "        self.feature_layer = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden_size),  # First layer can be standard Linear\n",
        "            nn.ReLU(),\n",
        "            NoisyLinear(hidden_size, hidden_size),  # Subsequent layers are noisy\n",
        "            nn.ReLU(),\n",
        "        ).to(\n",
        "            self.device\n",
        "        )  # Ensure layers are moved to the correct device\n",
        "\n",
        "        # --- Dueling Architecture Streams ---\n",
        "        # 1. Value Stream: Estimates V(s) - output shape [batch_size, num_atoms]\n",
        "        self.value_stream = nn.Sequential(\n",
        "            NoisyLinear(hidden_size, hidden_size // 2),  # Smaller layer for value\n",
        "            nn.ReLU(),\n",
        "            NoisyLinear(hidden_size // 2, num_atoms),\n",
        "        ).to(self.device)\n",
        "\n",
        "        # 2. Advantage Stream: Estimates A(s, a) - output shape [batch_size, action_size * num_atoms]\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            NoisyLinear(hidden_size, hidden_size // 2),  # Smaller layer for advantage\n",
        "            nn.ReLU(),\n",
        "            NoisyLinear(hidden_size // 2, action_size * num_atoms),\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Move the entire module to the specified device AFTER initializing layers\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Ensure input tensor is on the correct device\n",
        "        if x.device != self.device:\n",
        "            x = x.to(self.device)\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        # 1. Pass through shared feature layer\n",
        "        features = self.feature_layer(x)  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "        # 2. Pass features through value and advantage streams\n",
        "        value_logits = self.value_stream(features)  # Shape: [batch_size, num_atoms]\n",
        "        advantage_logits = self.advantage_stream(\n",
        "            features\n",
        "        )  # Shape: [batch_size, action_size * num_atoms]\n",
        "\n",
        "        # 3. Reshape streams for combination\n",
        "        # Reshape value to be broadcastable: [batch_size, 1, num_atoms]\n",
        "        value_logits = value_logits.view(batch_size, 1, self.num_atoms)\n",
        "        # Reshape advantage: [batch_size, action_size, num_atoms]\n",
        "        advantage_logits = advantage_logits.view(\n",
        "            batch_size, self.action_size, self.num_atoms\n",
        "        )\n",
        "\n",
        "        # 4. Combine Value and Advantage streams (Dueling formula applied to logits)\n",
        "        # Q(s, a) = V(s) + (A(s, a) - mean(A(s, .)))\n",
        "        # Calculate mean advantage across actions for each atom\n",
        "        mean_advantage_logits = advantage_logits.mean(\n",
        "            dim=1, keepdim=True\n",
        "        )  # Shape: [batch_size, 1, num_atoms]\n",
        "\n",
        "        # Combine using broadcasting\n",
        "        q_logits = value_logits + (\n",
        "            advantage_logits - mean_advantage_logits\n",
        "        )  # Shape: [batch_size, action_size, num_atoms]\n",
        "\n",
        "        # 5. Apply Softmax to get the probability distribution over atoms for each action\n",
        "        # Softmax is applied along the last dimension (atoms)\n",
        "        dist = torch.softmax(\n",
        "            q_logits, dim=2\n",
        "        )  # Shape: [batch_size, action_size, num_atoms]\n",
        "\n",
        "        return dist\n",
        "\n",
        "    def reset_noise(self):\n",
        "\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, NoisyLinear):\n",
        "                module.reset_noise()\n",
        "\n",
        "    def get_q_values(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        # Ensure support is on the correct device\n",
        "        if self.support.device != self.device:\n",
        "            self.support = self.support.to(self.device)\n",
        "\n",
        "        dist = self.forward(state)\n",
        "        q_values = (dist * self.support).sum(dim=2) \n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2R5f5yMZYfES"
      },
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, capacity, alpha=0.6, beta=0.4, n_step=3, gamma=0.99):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.n_step = n_step\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.buffer = []\n",
        "        self.n_step_buffer = deque(maxlen=n_step)\n",
        "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
        "        self.position = 0\n",
        "        self._log_frequency = 5\n",
        "\n",
        "    def _get_n_step_info(self):\n",
        "        \n",
        "        reward, next_state, done = self.n_step_buffer[-1][-3:]\n",
        "\n",
        "        for transition in reversed(list(self.n_step_buffer)[:-1]):\n",
        "            r, n_s, d = transition[-3:]\n",
        "            reward = r + self.gamma * reward * (1 - d)\n",
        "            if d:\n",
        "                next_state, done = n_s, d\n",
        "\n",
        "        state, action = self.n_step_buffer[0][:2]\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \n",
        "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        if len(self.n_step_buffer) < self.n_step:\n",
        "            return\n",
        "\n",
        "        state, action, reward, next_state, done = self._get_n_step_info()\n",
        "\n",
        "        # --- LOGGING ---\n",
        "        # Log the calculated N-step reward periodically to avoid flooding the console\n",
        "        self._push_count += 1\n",
        "        if self._push_count % self._log_frequency == 0:\n",
        "            # Get the sequence of immediate rewards that went into this calculation\n",
        "            immediate_rewards = [t[2] for t in self.n_step_buffer]\n",
        "            print(\n",
        "                f\"[Buffer Push Step {self._push_count}] Storing N-Step Transition: \"\n",
        "                f\"Immediate Rewards: {[f'{r:.3f}' for r in immediate_rewards]} -> \"\n",
        "                f\"N-Step Reward: {reward:.4f} | N-Step Done: {done}\"\n",
        "            )\n",
        "        # --- END LOGGING ---\n",
        "\n",
        "        max_priority = self.priorities.max() if len(self.buffer) > 0 else 1.0\n",
        "\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "        else:\n",
        "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "\n",
        "        self.priorities[self.position] = max_priority\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer) == 0:\n",
        "            return None\n",
        "\n",
        "        probs = self.priorities[: len(self.buffer)] ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
        "        samples = [self.buffer[idx] for idx in indices]\n",
        "\n",
        "        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        return samples, indices, weights\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TglYvlcVYfET"
      },
      "outputs": [],
      "source": [
        "class RainbowDQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_size,\n",
        "        action_size,\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        # Hyperparameters\n",
        "        v_min=-50.0,  \n",
        "        v_max=50.0,  \n",
        "        num_atoms=101,\n",
        "        hidden_size=128,\n",
        "        buffer_capacity=100000,\n",
        "        batch_size=32,\n",
        "        gamma=0.99,\n",
        "        n_step=3,\n",
        "        per_alpha=0.6,\n",
        "        per_beta=0.4,\n",
        "        target_update=1000, \n",
        "        learning_rate=1e-3, \n",
        "    ):\n",
        "        self.device = torch.device(device) \n",
        "        self.action_size = action_size\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.n_step = n_step\n",
        "        self.target_update = target_update\n",
        "        self.steps_done = 0\n",
        "\n",
        "        # Distributional RL parameters\n",
        "        self.v_min = v_min\n",
        "        self.v_max = v_max\n",
        "        self.num_atoms = num_atoms\n",
        "        self.support = torch.linspace(v_min, v_max, num_atoms).to(self.device)\n",
        "        self.delta_z = (v_max - v_min) / (num_atoms - 1)\n",
        "\n",
        "        # Create networks with specified parameters\n",
        "        self.policy_net = RainbowDQN(\n",
        "            state_size, action_size, num_atoms, v_min, v_max, hidden_size, self.device\n",
        "        ).to(self.device)\n",
        "        self.target_net = RainbowDQN(\n",
        "            state_size, action_size, num_atoms, v_min, v_max, hidden_size, self.device\n",
        "        ).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        # Create optimizer\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Create replay buffer with N-step returns\n",
        "        self.memory = PrioritizedReplayBuffer(\n",
        "            buffer_capacity, per_alpha, per_beta, n_step, gamma\n",
        "        )\n",
        "\n",
        "        self.beta_start = per_beta\n",
        "        self.beta_frames = 100000\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Ensure state is a float tensor and on the correct device\n",
        "            if not isinstance(state, torch.Tensor):\n",
        "                s = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            else:\n",
        "                s = state.float().unsqueeze(0).to(self.device)\n",
        "\n",
        "            if s.shape[1] != self.policy_net.state_size:\n",
        "                raise ValueError(\n",
        "                    f\"State shape mismatch in select_action. Expected {self.policy_net.state_size}, got {s.shape[1]}\"\n",
        "                )\n",
        "\n",
        "            # Get expected Q-values using the helper function\n",
        "            q_values = self.policy_net.get_q_values(s)  # Shape: [1, action_size]\n",
        "            # Select action with the highest expected Q-value\n",
        "            return q_values.argmax(1).item()\n",
        "\n",
        "\n",
        "    def _categorical_projection(self, next_dist_target: torch.Tensor, rewards: torch.Tensor,\n",
        "                                dones: torch.Tensor, next_action: torch.Tensor) -> torch.Tensor:\n",
        "        \n",
        "        batch_size = next_dist_target.size(0)\n",
        "        # Ensure inputs are on the correct device\n",
        "        rewards = rewards.to(self.device)\n",
        "        dones = dones.to(self.device)\n",
        "        next_action = next_action.to(self.device)\n",
        "\n",
        "        # Expand reward, done, and next_action tensors for broadcasting\n",
        "        rewards = rewards.view(batch_size, 1) # [B, 1]\n",
        "        dones_mask = (~dones).float().view(batch_size, 1) # [B, 1], 1.0 if not done, 0.0 if done\n",
        "\n",
        "        # Gather the next state distributions corresponding to the selected best next actions\n",
        "        next_action_expanded = next_action.view(batch_size, 1, 1).expand(-1, -1, self.num_atoms) # [B, 1, N]\n",
        "        next_dist_best_action = next_dist_target.gather(1, next_action_expanded).squeeze(1) # [B, N]\n",
        "\n",
        "        # --- Calculate Projected Atoms for Non-Terminal States ---\n",
        "        # Tz = R + gamma^N * z * (1 - done)\n",
        "        Tz = rewards + (self.gamma ** self.n_step) * self.support.view(1, -1) * dones_mask # [B, N]\n",
        "        # Clip projected atoms to [Vmin, Vmax]\n",
        "        Tz = Tz.clamp(self.v_min, self.v_max)\n",
        "\n",
        "        # Calculate the fractional bin indices 'b'\n",
        "        b = (Tz - self.v_min) / self.delta_z # [B, N]\n",
        "\n",
        "        # Determine the lower and upper integer bin indices\n",
        "        lower_bound = b.floor().long() # [B, N]\n",
        "        upper_bound = b.ceil().long()  # [B, N]\n",
        "\n",
        "        # --- Distribute Probability Mass (Vectorized using scatter_add_) ---\n",
        "        # Initialize the target distribution tensor\n",
        "        target_dist = torch.zeros(batch_size, self.num_atoms, device=self.device) # [B, N]\n",
        "\n",
        "        # Calculate weights for lower and upper bins based on distance\n",
        "        # Weight assigned to lower bin = (distance from upper bin) * original_probability\n",
        "        weight_l = (upper_bound.float() - b) * next_dist_best_action # [B, N]\n",
        "        # Weight assigned to upper bin = (distance from lower bin) * original_probability\n",
        "        weight_u = (b - lower_bound.float()) * next_dist_best_action # [B, N]\n",
        "\n",
        "        target_dist.scatter_add_(1, lower_bound.clamp(0, self.num_atoms - 1), weight_l)\n",
        "        target_dist.scatter_add_(1, upper_bound.clamp(0, self.num_atoms - 1), weight_u)\n",
        "\n",
        "        done_indices = torch.where(dones)[0]\n",
        "        num_done = len(done_indices)\n",
        "\n",
        "        if num_done > 0:\n",
        "\n",
        "            Tz_done = rewards[done_indices].clamp(self.v_min, self.v_max) # Shape: [num_done, 1]\n",
        "            b_done = (Tz_done - self.v_min) / self.delta_z # Shape: [num_done, 1]\n",
        "            l_done = b_done.floor().long() # Shape: [num_done, 1]\n",
        "            u_done = b_done.ceil().long() # Shape: [num_done, 1]\n",
        "\n",
        "            target_dist_done = torch.zeros(num_done, self.num_atoms, device=self.device)\n",
        "\n",
        "            weight_ld = (u_done.float() - b_done).squeeze(1) # Shape: [num_done]\n",
        "            weight_ud = (b_done - l_done.float()).squeeze(1) # Shape: [num_done]\n",
        "\n",
        "            eq_mask = (l_done == u_done).squeeze(1) # Shape: [num_done]\n",
        "            weight_ld[eq_mask] = 0.0 # Set weights explicitly for clarity where l==u\n",
        "            weight_ud[eq_mask] = 0.0\n",
        "\n",
        "            target_dist_done.scatter_(1, l_done.clamp(0, self.num_atoms - 1), weight_ld.unsqueeze(1))\n",
        "            target_dist_done.scatter_add_(1, u_done.clamp(0, self.num_atoms - 1), weight_ud.unsqueeze(1))\n",
        "\n",
        "            if torch.any(eq_mask):\n",
        "                l_done_eq = l_done[eq_mask].clamp(0, self.num_atoms - 1)\n",
        "\n",
        "                src_ones = torch.ones(l_done_eq.size(0), device=self.device).unsqueeze(1)\n",
        "                target_dist_done.scatter_(1, l_done_eq, src_ones) \n",
        "\n",
        "\n",
        "            # Replace the rows in the main target_dist with these calculated done distributions\n",
        "            target_dist[done_indices] = target_dist_done\n",
        "\n",
        "        # Optional: Normalize rows to ensure they sum to 1 due to potential floating point inaccuracies\n",
        "        # target_dist /= target_dist.sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
        "\n",
        "        return target_dist\n",
        "\n",
        "    def optimize_model(self):\n",
        "        \n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return None  # Not enough samples yet\n",
        "\n",
        "\n",
        "        beta = min(\n",
        "            1.0,\n",
        "            self.beta_start\n",
        "            + self.steps_done * (1.0 - self.beta_start) / self.beta_frames,\n",
        "        )\n",
        "\n",
        "        transitions, indices, weights = self.memory.sample(\n",
        "            self.batch_size\n",
        "        )\n",
        "        batch = list(zip(*transitions))\n",
        "\n",
        "        \n",
        "        states_np = np.array(batch[0], dtype=np.float32)\n",
        "        actions_np = np.array(batch[1], dtype=np.int64)\n",
        "        rewards_np = np.array(batch[2], dtype=np.float32)  # These are N-step rewards\n",
        "        next_states_np = np.array(batch[3], dtype=np.float32)  # This is state S_{t+N}\n",
        "        dones_np = np.array(batch[4], dtype=bool)  # Done flags for S_{t+N}\n",
        "\n",
        "        state_batch = torch.from_numpy(states_np).to(self.device)\n",
        "        action_batch = torch.from_numpy(actions_np).to(self.device)  # [B]\n",
        "        reward_batch = torch.from_numpy(rewards_np).to(self.device)  # [B]\n",
        "        next_state_batch = torch.from_numpy(next_states_np).to(self.device)\n",
        "        done_batch = torch.from_numpy(dones_np).to(self.device)  # [B]\n",
        "        weights = (\n",
        "            torch.from_numpy(np.array(weights, dtype=np.float32))\n",
        "            .to(self.device)\n",
        "            .unsqueeze(1)\n",
        "        )  # [B, 1] for broadcasting loss\n",
        "\n",
        "        # --- Target Calculation ---\n",
        "        with torch.no_grad():\n",
        "            next_dist_target = self.target_net(next_state_batch)  # [B, A, N]\n",
        "            next_q_target = (next_dist_target * self.support).sum(dim=2)  # [B, A]\n",
        "            next_action = next_q_target.argmax(dim=1)  # [B]\n",
        "            \n",
        "            target_dist = self._categorical_projection(\n",
        "                next_dist_target,  # Distribution from target net [B, A, N]\n",
        "                reward_batch,  # N-step Rewards [B]\n",
        "                done_batch,  # Done flags for S_{t+N} [B]\n",
        "                next_action,  # Best action in S_{t+N} selected by target net [B]\n",
        "            )\n",
        "        current_dist = self.policy_net(state_batch)  # [B, A, N]\n",
        "\n",
        "        action_batch_expanded = action_batch.view(self.batch_size, 1, 1).expand(\n",
        "            -1, -1, self.num_atoms\n",
        "        )  # [B, 1, N]\n",
        "        current_dist_taken_action = current_dist.gather(\n",
        "            1, action_batch_expanded\n",
        "        ).squeeze(\n",
        "            1\n",
        "        )  # [B, N]\n",
        "\n",
        "        # Compute cross-entropy loss between target and current distributions\n",
        "        # Add small epsilon for numerical stability before log\n",
        "        loss = -(target_dist * torch.log(current_dist_taken_action + 1e-8)).sum(\n",
        "            dim=1\n",
        "        )  # [B]\n",
        "\n",
        "        # Apply PER weights: loss = loss * weights\n",
        "        # Loss shape is [B], weights shape is [B, 1], squeeze weights or unsqueeze loss\n",
        "        weighted_loss = (\n",
        "            loss * weights.squeeze(1)\n",
        "        ).mean()  # Calculate the mean weighted loss\n",
        "\n",
        "        # --- Update Priorities in PER Buffer ---\n",
        "        new_priorities = loss.abs().detach().cpu().numpy() + 1e-6\n",
        "        self.memory.update_priorities(indices, new_priorities)\n",
        "\n",
        "        # --- Optimize Policy Network ---\n",
        "        self.optimizer.zero_grad()\n",
        "        weighted_loss.backward()\n",
        "        # Clip gradients to prevent explosions\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # --- Reset Noise ---\n",
        "        # Reset noise in NoisyLinear layers (for both networks if target uses them)\n",
        "        # Important for exploration when using Noisy Nets\n",
        "        self.policy_net.reset_noise()\n",
        "        self.target_net.reset_noise()\n",
        "\n",
        "        # --- Update Target Network --- (Soft update often more stable, but periodic hard update is simpler)\n",
        "        self.steps_done += 1  # Increment optimization step counter\n",
        "        if self.steps_done % self.target_update == 0:\n",
        "            print(f\"--- Updating target network at step {self.steps_done} ---\")\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        return weighted_loss.item()  # Return loss value for logging if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kPknKHKKYfET"
      },
      "outputs": [],
      "source": [
        "# Example modification in train_agent loop:\n",
        "def train_agent(\n",
        "    env, agent, num_episodes=1000, max_steps_per_episode=10000\n",
        "):  # Add max steps\n",
        "    returns_history = []\n",
        "    losses_history = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_return = 0\n",
        "        episode_losses = []\n",
        "        done = False\n",
        "        steps_in_episode = 0  # Track steps within episode\n",
        "\n",
        "        while not done and steps_in_episode < max_steps_per_episode:\n",
        "            # Select action\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Store transition in memory (using N-step buffer)\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "            # Move to next state\n",
        "            state = next_state\n",
        "            episode_return += reward\n",
        "\n",
        "            # Optimize model (only if buffer is large enough)\n",
        "            loss_val = agent.optimize_model()  # optimize_model increments steps_done\n",
        "            if loss_val is not None:\n",
        "                episode_losses.append(loss_val)\n",
        "\n",
        "            steps_in_episode += 1\n",
        "\n",
        "        returns_history.append(\n",
        "            env.portfolio_value\n",
        "        )  # Store final portfolio value or total return\n",
        "        avg_loss = np.mean(episode_losses) if episode_losses else 0\n",
        "        losses_history.append(avg_loss)\n",
        "        print(\n",
        "            f\"Episode {episode + 1}/{num_episodes}, \"\n",
        "            f\"Steps: {steps_in_episode}, \"\n",
        "            f\"Total Steps: {agent.steps_done}, \"\n",
        "            f\"Return: {env.portfolio_value:.2f}, \"  # Print final portfolio value\n",
        "            f\"Avg Loss: {avg_loss:.4f}\"\n",
        "        )\n",
        "\n",
        "    return returns_history, losses_history, agent  # Return losses too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_agent_on_data(\n",
        "    agent: RainbowDQNAgent, eval_data: pd.DataFrame, initial_balance: float\n",
        ") -> tuple[list, list]:\n",
        "    \"\"\"\n",
        "    Simulates the trained agent's trading on evaluation data without learning.\n",
        "    This is a core part of the evaluation process.\n",
        "\n",
        "    Args:\n",
        "        agent: The trained RainbowDQNAgent instance.\n",
        "        eval_data: Preprocessed DataFrame containing evaluation data.\n",
        "        initial_balance: Starting balance for the simulation.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - portfolio_values (list): History of portfolio value at each step.\n",
        "            - actions_taken (list): History of actions taken (0, 1, or 2).\n",
        "    \"\"\"\n",
        "    print(f\"Simulating agent on evaluation data...\")\n",
        "    # Create a temporary environment just for this simulation run\n",
        "    eval_env = TradingEnvironment(eval_data, initial_balance=initial_balance)\n",
        "    state = eval_env.reset()\n",
        "    done = False\n",
        "    portfolio_values = [initial_balance]  # Start with initial balance\n",
        "    actions_taken = []\n",
        "\n",
        "    # Ensure agent is in evaluation mode (disables noise, dropout etc.)\n",
        "    agent.policy_net.eval()\n",
        "\n",
        "    while not done:\n",
        "        # Get action without calculating gradients\n",
        "        with torch.no_grad():\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "        # Step through the environment\n",
        "        next_state, reward, done = eval_env.step(action)\n",
        "\n",
        "        # Record results for this step\n",
        "        portfolio_values.append(eval_env.portfolio_value)\n",
        "        actions_taken.append(action)\n",
        "        state = next_state\n",
        "\n",
        "    # Set agent back to training mode (good practice if reusing agent object)\n",
        "    agent.policy_net.train()\n",
        "    print(\n",
        "        f\"Agent simulation complete. Final portfolio value: {portfolio_values[-1]:.2f}\"\n",
        "    )\n",
        "    return portfolio_values, actions_taken\n",
        "\n",
        "\n",
        "def simulate_buy_and_hold(\n",
        "    eval_data: pd.DataFrame, initial_balance: float\n",
        ") -> tuple[list, float]:\n",
        "    \"\"\"\n",
        "    Simulates a simple Buy and Hold strategy on the evaluation data.\n",
        "\n",
        "    Args:\n",
        "        eval_data: Preprocessed DataFrame containing evaluation data (needs 'Close').\n",
        "        initial_balance: Starting balance for the simulation.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - portfolio_values (list): History of portfolio value at each step.\n",
        "            - total_return_pct (float): The total return percentage.\n",
        "    \"\"\"\n",
        "    print(\"Simulating Buy and Hold strategy...\")\n",
        "    if eval_data.empty:\n",
        "        print(\"Warning: Cannot simulate Buy and Hold on empty data.\")\n",
        "        return [initial_balance], 0.0\n",
        "\n",
        "    start_price = eval_data[\"Close\"].iloc[0]\n",
        "    end_price = eval_data[\"Close\"].iloc[-1]\n",
        "\n",
        "    # Calculate number of shares bought at the start\n",
        "    num_shares = initial_balance // start_price\n",
        "    cash_left = initial_balance - (num_shares * start_price)\n",
        "\n",
        "    # Calculate daily portfolio values (value of shares + leftover cash)\n",
        "    daily_values = (eval_data[\"Close\"] * num_shares) + cash_left\n",
        "    portfolio_values = [\n",
        "        initial_balance\n",
        "    ] + daily_values.tolist()  # Add initial balance at start\n",
        "\n",
        "    # Calculate final metrics\n",
        "    final_value = portfolio_values[-1]\n",
        "    total_return_pct = ((final_value - initial_balance) / initial_balance) * 100\n",
        "\n",
        "    print(\n",
        "        f\"Buy & Hold simulation complete. Final portfolio value: {final_value:.2f}, Total Return: {total_return_pct:.2f}%\"\n",
        "    )\n",
        "    return portfolio_values, total_return_pct\n",
        "\n",
        "\n",
        "def calculate_performance_metrics(\n",
        "    portfolio_values: list, risk_free_rate: float = 0.0\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Calculates key performance metrics from a list of portfolio values.\n",
        "\n",
        "    Args:\n",
        "        portfolio_values: List of portfolio values over time.\n",
        "        risk_free_rate: Annual risk-free rate for Sharpe Ratio calculation.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing: 'Sharpe Ratio', 'Max Drawdown'.\n",
        "    \"\"\"\n",
        "    metrics = {\"Sharpe Ratio\": 0.0, \"Max Drawdown\": 0.0}\n",
        "    if len(portfolio_values) < 2:\n",
        "        print(\"Warning: Need at least 2 portfolio values to calculate metrics.\")\n",
        "        return metrics\n",
        "\n",
        "    values_series = pd.Series(portfolio_values)\n",
        "    daily_returns = values_series.pct_change().dropna()\n",
        "\n",
        "    # Sharpe Ratio\n",
        "    if not daily_returns.empty and daily_returns.std() != 0:\n",
        "        excess_returns = daily_returns - risk_free_rate / 252\n",
        "        sharpe = excess_returns.mean() / excess_returns.std()\n",
        "        metrics[\"Sharpe Ratio\"] = sharpe * np.sqrt(252)  # Annualized\n",
        "\n",
        "    # Max Drawdown\n",
        "    cumulative_max = values_series.cummax()\n",
        "    drawdown = ((values_series - cumulative_max) / cumulative_max) * 100  # In percent\n",
        "    metrics[\"Max Drawdown\"] = drawdown.min() if not drawdown.empty else 0.0\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_drawdown_series(portfolio_values: list) -> pd.Series:\n",
        "    \"\"\"Calculates the historical drawdown series.\"\"\"\n",
        "    portfolio_values_series = pd.Series(portfolio_values)\n",
        "    if len(portfolio_values_series) < 2:\n",
        "        return pd.Series(dtype=float)\n",
        "    cumulative_max = portfolio_values_series.cummax()\n",
        "    drawdown = (\n",
        "        (portfolio_values_series - cumulative_max) / cumulative_max.replace(0, 1e-9)\n",
        "    ) * 100  # Avoid div by zero if starts at 0\n",
        "    return drawdown.fillna(0)  # Fill initial NaN\n",
        "\n",
        "\n",
        "def plot_portfolio_value(agent_values, bnh_values, initial_balance, index, save_path):\n",
        "    \"\"\"Plots Agent vs Buy&Hold Portfolio Value over Time.\"\"\"\n",
        "    print(f\"Generating Portfolio Value plot...\")\n",
        "    if (\n",
        "        not agent_values\n",
        "        or not bnh_values\n",
        "        or len(agent_values) <= 1\n",
        "        or len(bnh_values) <= 1\n",
        "    ):\n",
        "        print(\"Skipping portfolio value plot due to insufficient data.\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    # Plot lines starting from step 1 to align with index\n",
        "    ax.plot(\n",
        "        index,\n",
        "        agent_values[1:],\n",
        "        label=\"Agent Portfolio\",\n",
        "        color=\"tab:blue\",\n",
        "        linewidth=1.5,\n",
        "    )\n",
        "    ax.plot(\n",
        "        index,\n",
        "        bnh_values[1:],\n",
        "        label=\"Buy & Hold Portfolio\",\n",
        "        color=\"tab:orange\",\n",
        "        linewidth=1.5,\n",
        "    )\n",
        "    ax.axhline(y=initial_balance, color=\"gray\", linestyle=\"--\", label=\"Initial Balance\")\n",
        "\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Portfolio Value\")\n",
        "    ax.set_title(\"Agent vs. Buy & Hold Portfolio Value (Test Set)\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, linestyle=\":\", alpha=0.6)\n",
        "    ax.ticklabel_format(style=\"plain\", axis=\"y\")\n",
        "    fig.autofmt_xdate()\n",
        "\n",
        "    try:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "        print(f\"Portfolio Value plot saved to {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving Portfolio Value plot: {e}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_drawdown(agent_values, bnh_values, index, save_path):\n",
        "    \"\"\"Plots Agent vs Buy&Hold Drawdown over Time.\"\"\"\n",
        "    print(f\"Generating Drawdown plot...\")\n",
        "    if (\n",
        "        not agent_values\n",
        "        or not bnh_values\n",
        "        or len(agent_values) <= 1\n",
        "        or len(bnh_values) <= 1\n",
        "    ):\n",
        "        print(\"Skipping drawdown plot due to insufficient data.\")\n",
        "        return\n",
        "\n",
        "    agent_drawdown = calculate_drawdown_series(agent_values)\n",
        "    bnh_drawdown = calculate_drawdown_series(bnh_values)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    # Plot drawdowns aligning with index\n",
        "    ax.plot(\n",
        "        index,\n",
        "        agent_drawdown[1:],\n",
        "        label=\"Agent Drawdown\",\n",
        "        color=\"tab:red\",\n",
        "        linewidth=1.5,\n",
        "    )\n",
        "    ax.plot(\n",
        "        index,\n",
        "        bnh_drawdown[1:],\n",
        "        label=\"Buy & Hold Drawdown\",\n",
        "        color=\"tab:purple\",\n",
        "        linewidth=1.5,\n",
        "    )\n",
        "    ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.7)\n",
        "\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Drawdown (%)\")\n",
        "    ax.set_title(\"Agent vs. Buy & Hold Drawdown (Test Set)\")\n",
        "    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "    ax.legend()\n",
        "    ax.grid(True, linestyle=\":\", alpha=0.6)\n",
        "    fig.autofmt_xdate()\n",
        "\n",
        "    try:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "        print(f\"Drawdown plot saved to {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving Drawdown plot: {e}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_actions(eval_data, actions_taken, save_path):\n",
        "    \"\"\"Plots Agent Buy/Sell Actions Overlayed on Price.\"\"\"\n",
        "    print(f\"Generating Actions plot...\")\n",
        "    if eval_data.empty or not actions_taken:\n",
        "        print(\"Skipping actions plot due to empty data.\")\n",
        "        return\n",
        "\n",
        "    # Align actions with the dates in eval_data\n",
        "    # actions_taken corresponds to decisions made *at the start* of the step\n",
        "    # Price plotted is usually the closing price *at the end* of the step\n",
        "    # We plot the action marker on the day the action was decided.\n",
        "    if len(actions_taken) != len(eval_data):\n",
        "        print(\n",
        "            f\"Warning: Length mismatch actions ({len(actions_taken)}) vs data ({len(eval_data)}). Adjusting.\"\n",
        "        )\n",
        "        min_len = min(len(actions_taken), len(eval_data))\n",
        "        eval_data_plot = eval_data.iloc[:min_len]\n",
        "        actions_plot = np.array(\n",
        "            actions_taken[:min_len]\n",
        "        )  # Convert to numpy array for boolean indexing\n",
        "    else:\n",
        "        eval_data_plot = eval_data\n",
        "        actions_plot = np.array(actions_taken)\n",
        "\n",
        "    buy_indices = np.where(actions_plot == 1)[0]\n",
        "    sell_indices = np.where(actions_plot == 2)[0]\n",
        "\n",
        "    buy_signals_dates = eval_data_plot.index[buy_indices]\n",
        "    sell_signals_dates = eval_data_plot.index[sell_indices]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 7))\n",
        "    ax.plot(\n",
        "        eval_data_plot.index,\n",
        "        eval_data_plot[\"Close\"],\n",
        "        label=\"Close Price\",\n",
        "        color=\"black\",\n",
        "        alpha=0.8,\n",
        "        linewidth=1,\n",
        "    )\n",
        "\n",
        "    if len(buy_signals_dates) > 0:\n",
        "        ax.plot(\n",
        "            buy_signals_dates,\n",
        "            eval_data_plot.loc[buy_signals_dates][\"Close\"],\n",
        "            \"^\",\n",
        "            markersize=8,\n",
        "            color=\"green\",\n",
        "            label=\"Buy Signal\",\n",
        "            alpha=0.9,\n",
        "            linestyle=\"None\",\n",
        "        )\n",
        "    if len(sell_signals_dates) > 0:\n",
        "        ax.plot(\n",
        "            sell_signals_dates,\n",
        "            eval_data_plot.loc[sell_signals_dates][\"Close\"],\n",
        "            \"v\",\n",
        "            markersize=8,\n",
        "            color=\"red\",\n",
        "            label=\"Sell Signal\",\n",
        "            alpha=0.9,\n",
        "            linestyle=\"None\",\n",
        "        )\n",
        "\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Price\")\n",
        "    ax.set_title(\"Agent Trading Actions on Price (Test Set)\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, linestyle=\":\", alpha=0.6)\n",
        "    fig.autofmt_xdate()\n",
        "\n",
        "    try:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "        print(f\"Actions plot saved to {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving Actions plot: {e}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def plot_returns_histogram(agent_values, bnh_values, save_path, bins=50):\n",
        "    \"\"\"Plots Histogram of Daily Returns for Agent vs Buy&Hold.\"\"\"\n",
        "    print(f\"Generating Returns Histogram plot...\")\n",
        "    if len(agent_values) <= 1 or len(bnh_values) <= 1:\n",
        "        print(\"Skipping returns histogram plot due to insufficient data.\")\n",
        "        return\n",
        "\n",
        "    agent_returns = pd.Series(agent_values).pct_change().dropna() * 100  # In percentage\n",
        "    bnh_returns = pd.Series(bnh_values).pct_change().dropna() * 100  # In percentage\n",
        "\n",
        "    if agent_returns.empty and bnh_returns.empty:\n",
        "        print(\"No returns data to plot histogram.\")\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.hist(\n",
        "        agent_returns,\n",
        "        bins=bins,\n",
        "        alpha=0.7,\n",
        "        label=\"Agent Daily Returns\",\n",
        "        color=\"tab:blue\",\n",
        "        density=True,\n",
        "    )\n",
        "    ax.hist(\n",
        "        bnh_returns,\n",
        "        bins=bins,\n",
        "        alpha=0.7,\n",
        "        label=\"Buy & Hold Daily Returns\",\n",
        "        color=\"tab:orange\",\n",
        "        density=True,\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(\"Daily Return (%)\")\n",
        "    ax.set_ylabel(\"Density\")\n",
        "    ax.set_title(\"Distribution of Daily Returns (Test Set)\")\n",
        "    ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "    ax.legend()\n",
        "    ax.grid(True, axis=\"y\", linestyle=\":\", alpha=0.6)\n",
        "\n",
        "    try:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "        print(f\"Returns Histogram plot saved to {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving Returns Histogram plot: {e}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def print_training_charts(returns_history, losses_history, initial_balance, save_path):\n",
        "    # Generates and saves the plot showing training progress.\n",
        "    # Renamed from print_charts to avoid confusion with eval plots.\n",
        "    print(\"Generating training performance plot...\")\n",
        "    if not returns_history or not losses_history:\n",
        "        print(\"Skipping training plot as no training history was generated.\")\n",
        "        return\n",
        "\n",
        "    cumulative_returns_pct = [\n",
        "        ((final_value - initial_balance) / initial_balance) * 100\n",
        "        for final_value in returns_history\n",
        "    ]\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
        "    color_return = \"tab:red\"\n",
        "    ax1.set_xlabel(\"Episode\")\n",
        "    ax1.set_ylabel(\"Cumulative Return (%)\", color=color_return)\n",
        "    ax1.plot(\n",
        "        cumulative_returns_pct,\n",
        "        color=color_return,\n",
        "        label=\"Cumulative Return %\",\n",
        "        linewidth=1.5,\n",
        "    )\n",
        "    ax1.tick_params(axis=\"y\", labelcolor=color_return)\n",
        "    ax1.axhline(y=0, color=\"gray\", linestyle=\"--\", label=\"0% Return\")\n",
        "    ax1.legend(loc=\"upper left\")\n",
        "    ax1.grid(True, axis=\"y\", linestyle=\":\", alpha=0.6)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color_loss = \"tab:blue\"\n",
        "    ax2.set_ylabel(\"Average Loss\", color=color_loss)\n",
        "    ax2.plot(\n",
        "        losses_history, color=color_loss, alpha=0.7, label=\"Avg Loss\", linewidth=1.5\n",
        "    )\n",
        "    ax2.tick_params(axis=\"y\", labelcolor=color_loss)\n",
        "    ax2.legend(loc=\"upper right\")\n",
        "    plt.title(\"Training Performance: Cumulative Return and Average Loss per Episode\")\n",
        "    fig.tight_layout()\n",
        "\n",
        "    try:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "        print(f\"Training plot saved successfully to {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving training plot: {e}\")\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "# --- Top-Level Evaluation Orchestration Function ---\n",
        "def run_evaluation(\n",
        "    agent: RainbowDQNAgent, eval_data_path: str, initial_balance: float, output_dir: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads evaluation data, runs agent and baseline simulations,\n",
        "    calculates metrics, prints results, and generates evaluation plots.\n",
        "\n",
        "    Args:\n",
        "        agent: The trained agent instance.\n",
        "        eval_data_path: Path to the evaluation data CSV file.\n",
        "        initial_balance: Starting balance for evaluation simulations.\n",
        "        output_dir: Directory to save the evaluation plots.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 30 + \" STARTING EVALUATION \" + \"=\" * 30)\n",
        "\n",
        "    # --- Load and Prepare Evaluation Data ---\n",
        "    try:\n",
        "        eval_data_raw = pd.read_csv(eval_data_path, index_col=\"Date\", parse_dates=True)\n",
        "        print(f\"Loaded evaluation data from {eval_data_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Evaluation data file not found at {eval_data_path}\")\n",
        "        print(\"Evaluation skipped.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading evaluation data: {e}\")\n",
        "        print(\"Evaluation skipped.\")\n",
        "        return\n",
        "\n",
        "    # Calculate indicators and drop NaNs for the evaluation period\n",
        "    eval_data = calculate_technical_indicators(eval_data_raw.copy())\n",
        "    eval_data.dropna(inplace=True)\n",
        "\n",
        "    if eval_data.empty:\n",
        "        print(\n",
        "            \"Error: Evaluation data is empty after preprocessing. Evaluation skipped.\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    print(f\"Evaluation data range: {eval_data.index.min()} to {eval_data.index.max()}\")\n",
        "    print(f\"Evaluation data shape: {eval_data.shape}\")\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # --- Run Simulations ---\n",
        "    # Agent Simulation\n",
        "    agent_portfolio_values, agent_actions = simulate_agent_on_data(\n",
        "        agent, eval_data, initial_balance\n",
        "    )\n",
        "    # Buy and Hold Simulation\n",
        "    bnh_portfolio_values, bnh_total_return_pct = simulate_buy_and_hold(\n",
        "        eval_data, initial_balance\n",
        "    )\n",
        "\n",
        "    # --- Calculate Metrics ---\n",
        "    # Agent Metrics\n",
        "    agent_final_value = agent_portfolio_values[-1]\n",
        "    agent_total_return_pct = (\n",
        "        (agent_final_value - initial_balance) / initial_balance\n",
        "    ) * 100\n",
        "    agent_metrics = calculate_performance_metrics(agent_portfolio_values)\n",
        "\n",
        "    # BnH Metrics\n",
        "    bnh_metrics = calculate_performance_metrics(bnh_portfolio_values)\n",
        "\n",
        "    # --- Print Comparison Table ---\n",
        "    print(\"\\n--- Evaluation Results Comparison ---\")\n",
        "    print(\"| Metric                     | Rainbow DQN Agent | Buy-and-Hold Baseline |\")\n",
        "    print(\"| :------------------------- | :---------------- | :-------------------- |\")\n",
        "    print(\n",
        "        f\"| Total Return               | {agent_total_return_pct: >17.2f}% | {bnh_total_return_pct: >21.2f}% |\"\n",
        "    )\n",
        "    print(\n",
        "        f\"| Annualized Sharpe Ratio    | {agent_metrics['Sharpe Ratio']: >17.2f} | {bnh_metrics['Sharpe Ratio']: >21.2f} |\"\n",
        "    )\n",
        "    print(\n",
        "        f\"| Maximum Drawdown (MDD)     | {agent_metrics['Max Drawdown']: >17.2f}% | {bnh_metrics['Max Drawdown']: >21.2f}% |\"\n",
        "    )\n",
        "    print(\n",
        "        f\"| Initial Balance            | {initial_balance: >17,.0f} | {initial_balance: >21,.0f} |\"\n",
        "    )\n",
        "    print(\n",
        "        f\"| Final Portfolio Value      | {agent_final_value: >17,.2f} | {bnh_portfolio_values[-1]: >21,.2f} |\"\n",
        "    )\n",
        "\n",
        "    # --- Generate Evaluation Plots ---\n",
        "    print(\"\\n--- Generating Evaluation Plots ---\")\n",
        "    eval_index = eval_data.index  # Use dates from processed eval data\n",
        "\n",
        "    plot_portfolio_value(\n",
        "        agent_portfolio_values,\n",
        "        bnh_portfolio_values,\n",
        "        initial_balance,\n",
        "        eval_index,\n",
        "        os.path.join(output_dir, \"evaluation_portfolio_value.png\"),\n",
        "    )\n",
        "    plot_drawdown(\n",
        "        agent_portfolio_values,\n",
        "        bnh_portfolio_values,\n",
        "        eval_index,\n",
        "        os.path.join(output_dir, \"evaluation_drawdown.png\"),\n",
        "    )\n",
        "    plot_actions(\n",
        "        eval_data, agent_actions, os.path.join(output_dir, \"evaluation_actions.png\")\n",
        "    )\n",
        "    plot_returns_histogram(\n",
        "        agent_portfolio_values,\n",
        "        bnh_portfolio_values,\n",
        "        os.path.join(output_dir, \"evaluation_returns_histogram.png\"),\n",
        "    )\n",
        "\n",
        "    print(\"=\" * 30 + \" EVALUATION FINISHED \" + \"=\" * 30 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ObRCZk6dYfET",
        "outputId": "7f467d7e-b605-43e9-ccda-5701aaea9e68"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # np.random.seed(42)\n",
        "    # torch.manual_seed(42)\n",
        "    # random.seed(42)\n",
        "\n",
        "    # --- Define Paths and Parameters ---\n",
        "    TRAIN_DATA_PATH = \"/home/kartikeya.agrawal_ug25/RL_Final/data/train_data.csv\"  # Assumed available via collect_stock_data\n",
        "    EVAL_DATA_PATH = \"/home/kartikeya.agrawal_ug25/RL_Final/data/eval_data.csv\"\n",
        "    OUTPUT_DIR = \"/home/kartikeya.agrawal_ug25/RL_Final/output\"  # Directory for plots\n",
        "    NUM_TRAIN_EPISODES = 500  # Number of episodes for training\n",
        "    INITIAL_BALANCE = 100000  # Starting balance\n",
        "\n",
        "    # --- Data Loading and Preprocessing (for Training) ---\n",
        "    # collect_stock_data now loads from TRAIN_DATA_PATH\n",
        "    train_stock_data = collect_stock_data()\n",
        "    processed_train_data = calculate_technical_indicators(train_stock_data.copy())\n",
        "    processed_train_data.dropna(inplace=True)\n",
        "\n",
        "    print(\"Training Dataset shape:\", processed_train_data.shape)\n",
        "    print(\n",
        "        \"Training Date range:\",\n",
        "        processed_train_data.index.min(),\n",
        "        \"to\",\n",
        "        processed_train_data.index.max(),\n",
        "    )\n",
        "\n",
        "    # --- Create Training Environment and Agent ---\n",
        "    train_env = TradingEnvironment(\n",
        "        processed_train_data, initial_balance=INITIAL_BALANCE\n",
        "    )\n",
        "    state_size = train_env.state_size\n",
        "    action_size = 3\n",
        "\n",
        "    device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device_name}\")\n",
        "    agent = RainbowDQNAgent(state_size, action_size, device=device_name)\n",
        "\n",
        "\n",
        "    # --- Train the Agent ---\n",
        "    print(f\"Starting training for {NUM_TRAIN_EPISODES} episodes...\")\n",
        "    returns_history, losses_history, agent = train_agent(\n",
        "        train_env, agent, num_episodes=NUM_TRAIN_EPISODES\n",
        "    )\n",
        "\n",
        "    # --- Save Training Plot ---\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)  # Ensure output dir exists\n",
        "    training_plot_save_path = os.path.join(\n",
        "        OUTPUT_DIR, f\"training_performance_{NUM_TRAIN_EPISODES}.png\"\n",
        "    )\n",
        "    print_training_charts(\n",
        "        returns_history, losses_history, INITIAL_BALANCE, training_plot_save_path\n",
        "    )\n",
        "\n",
        "    # --- Run Full Evaluation ---\n",
        "    # Pass the *trained* agent, path to eval data, initial balance, and output dir\n",
        "    run_evaluation(agent, EVAL_DATA_PATH, INITIAL_BALANCE, OUTPUT_DIR)\n",
        "\n",
        "    print(\"Script finished.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
