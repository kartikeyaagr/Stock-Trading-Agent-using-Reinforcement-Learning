{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stock_data():\n",
    "    print(\"Loading pre-downloaded stock data...\")\n",
    "    DATA_FILE_PATH = \"reliance_data.csv\"\n",
    "    try:\n",
    "        stock_data = pd.read_csv(DATA_FILE_PATH, index_col=\"Date\", parse_dates=True)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {DATA_FILE_PATH}\")\n",
    "        print(\"Please run the data download script first.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from file: {e}\")\n",
    "        exit()\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method items of dict object at 0x1671eff80>\n"
     ]
    }
   ],
   "source": [
    "def calculate_technical_indicators(df):\n",
    "    # Calculate returns\n",
    "    df[\"Returns\"] = df[\"Close\"].pct_change()\n",
    "\n",
    "    # Calculate moving averages\n",
    "    df[\"SMA_20\"] = df[\"Close\"].rolling(window=20).mean()\n",
    "    df[\"SMA_50\"] = df[\"Close\"].rolling(window=50).mean()\n",
    "\n",
    "    # Calculate RSI\n",
    "    delta = df[\"Close\"].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Calculate MACD\n",
    "    exp1 = df[\"Close\"].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "    df[\"MACD\"] = exp1 - exp2\n",
    "    df[\"Signal_Line\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # Calculate Bollinger Bands\n",
    "    rolling_mean = df[\"Close\"].rolling(window=20).mean()\n",
    "    rolling_std = df[\"Close\"].rolling(window=20).std()\n",
    "    df[\"BB_middle\"] = rolling_mean\n",
    "    df[\"BB_upper\"] = rolling_mean + (2 * rolling_std)\n",
    "    df[\"BB_lower\"] = rolling_mean - (2 * rolling_std)\n",
    "\n",
    "    # Calculate volatility\n",
    "    df[\"Volatility\"] = df[\"Returns\"].rolling(window=20).std()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment:\n",
    "    def __init__(self, data: pd.DataFrame, initial_balance=100000):\n",
    "\n",
    "        if data.isnull().values.any():\n",
    "\n",
    "            nan_rows = data[data.isnull().any(axis=1)]\n",
    "            print(\n",
    "                \"Warning: DataFrame passed to TradingEnvironment contains NaN values.\"\n",
    "            )\n",
    "            print(\"Ensure .dropna() was called after calculating indicators.\")\n",
    "            print(\"First few rows with NaNs:\\n\", nan_rows.head())\n",
    "\n",
    "            raise ValueError(\n",
    "                \"DataFrame contains NaNs. Please clean before passing to Environment.\"\n",
    "            )\n",
    "\n",
    "        self.data = data.copy()\n",
    "        self.initial_balance = initial_balance\n",
    "\n",
    "        self.feature_columns = [\n",
    "            \"Returns\",\n",
    "            \"SMA_20\",\n",
    "            \"SMA_50\",\n",
    "            \"RSI\",\n",
    "            \"MACD\",\n",
    "            \"Signal_Line\",\n",
    "            \"BB_upper\",\n",
    "            \"BB_lower\",\n",
    "            \"Volatility\",\n",
    "        ]\n",
    "        self.price_column = \"Close\"\n",
    "\n",
    "        # --- Verify all required columns are present ---\n",
    "        required_cols_for_state = self.feature_columns + [self.price_column]\n",
    "        missing_cols = [\n",
    "            col for col in required_cols_for_state if col not in self.data.columns\n",
    "        ]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in input data: {missing_cols}\")\n",
    "\n",
    "        self.state_size = 3 + len(self.feature_columns)\n",
    "        print(f\"Environment initialized. State size: {self.state_size}\")\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.balance = self.initial_balance\n",
    "        self.current_step = 0  # Start from the first row (assuming NaNs are dropped)\n",
    "        self.position = 0  # Number of shares held\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.returns_history = []\n",
    "        # print(f\"Environment reset. Starting step: {self.current_step}, Index: {self.data.index[self.current_step]}\")\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "\n",
    "        if self.current_step >= len(self.data):\n",
    "            # Should not happen if step logic is correct, but safeguard\n",
    "            print(\n",
    "                f\"Warning: _get_state called at step {self.current_step} >= data length {len(self.data)}. Using last valid data.\"\n",
    "            )\n",
    "            self.current_step = len(self.data) - 1\n",
    "\n",
    "        current_data = self.data.iloc[self.current_step]\n",
    "        current_price = current_data[self.price_column]\n",
    "\n",
    "        # --- 1. Agent Status Features (Normalized) ---\n",
    "        position_value = self.position * current_price\n",
    "        normalized_position_value = (\n",
    "            position_value / self.portfolio_value\n",
    "            if self.portfolio_value > 1e-6\n",
    "            else 0.0\n",
    "        )  # Avoid div by zero\n",
    "        normalized_balance = self.balance / self.initial_balance\n",
    "        normalized_portfolio_value = self.portfolio_value / self.initial_balance\n",
    "\n",
    "        agent_state = [\n",
    "            normalized_position_value,\n",
    "            normalized_balance,\n",
    "            normalized_portfolio_value,\n",
    "        ]\n",
    "\n",
    "        # --- 2. Technical Indicator Features (Scaled/Normalized) ---\n",
    "        tech_state = []\n",
    "        # Use a small epsilon to prevent division by zero if price is exactly zero\n",
    "        epsilon = 1e-8\n",
    "        safe_price = current_price if abs(current_price) > epsilon else epsilon\n",
    "\n",
    "        # Iterate through defined feature columns and apply scaling/normalization\n",
    "        for col in self.feature_columns:\n",
    "            value = current_data[col]\n",
    "            if pd.isna(value):\n",
    "                # This shouldn't happen if data is pre-cleaned, but as a fallback\n",
    "                print(\n",
    "                    f\"Warning: NaN found in column '{col}' at step {self.current_step}. Replacing with 0.\"\n",
    "                )\n",
    "                value = 0.0\n",
    "\n",
    "            # Apply scaling/normalization based on the indicator type\n",
    "            if col == \"Returns\":\n",
    "                # Returns are often small, maybe scale slightly? Or keep as is. Let's keep as is for now.\n",
    "                scaled_value = value\n",
    "            elif col in [\n",
    "                \"SMA_20\",\n",
    "                \"SMA_50\",\n",
    "                \"BB_upper\",\n",
    "                \"BB_lower\",\n",
    "                \"MACD\",\n",
    "                \"Signal_Line\",\n",
    "            ]:\n",
    "                # Normalize price-based indicators relative to the current price\n",
    "                scaled_value = (value - current_price) / safe_price\n",
    "            elif col == \"RSI\":\n",
    "                # Scale RSI from [0, 100] to [0.0, 1.0]\n",
    "                scaled_value = value / 100.0\n",
    "            elif col == \"Volatility\":\n",
    "                # Volatility is a std dev (percentage), usually small. Keep as is for now.\n",
    "                scaled_value = value\n",
    "            else:\n",
    "                # Default for any unexpected columns (shouldn't happen with check in init)\n",
    "                scaled_value = value\n",
    "\n",
    "            tech_state.append(scaled_value)\n",
    "\n",
    "        # --- Combine states ---\n",
    "        state = agent_state + tech_state\n",
    "\n",
    "        # Final check for NaNs or Infs that might have slipped through calculations\n",
    "        state_np = np.array(state, dtype=np.float32)\n",
    "        if np.any(np.isnan(state_np)) or np.any(np.isinf(state_np)):\n",
    "            # print(f\"Warning: NaN/Inf detected in final state at step {self.current_step}. Replacing with 0.\")\n",
    "            # print(f\"Original state: {state}\")\n",
    "            state_np = np.nan_to_num(\n",
    "                state_np, nan=0.0, posinf=0.0, neginf=0.0\n",
    "            )  # Replace NaN/Inf with 0\n",
    "\n",
    "        # Ensure state size matches expectation\n",
    "        if len(state_np) != self.state_size:\n",
    "            raise RuntimeError(\n",
    "                f\"Internal Error: State size mismatch. Expected {self.state_size}, got {len(state_np)}\"\n",
    "            )\n",
    "\n",
    "        return state_np\n",
    "\n",
    "    # --- Keep the step, _calculate_cvar methods as they were in the previous fix ---\n",
    "    # (Make sure they use self.price_column instead of hardcoded \"Close\")\n",
    "    def step(self, action):\n",
    "        # Actions: 0 = hold, 1 = buy, 2 = sell\n",
    "\n",
    "        if self.current_step >= len(self.data) - 2:  # Need current and next price\n",
    "            # print(f\"Attempting to step beyond data bounds (step {self.current_step}). Ending episode.\")\n",
    "            current_state = self._get_state()  # Get the last valid state\n",
    "            return current_state, 0.0, True  # Return last state, 0 reward, done=True\n",
    "\n",
    "        current_data = self.data.iloc[self.current_step]\n",
    "        next_data = self.data.iloc[self.current_step + 1]\n",
    "\n",
    "        price = current_data[self.price_column]\n",
    "        next_price = next_data[self.price_column]\n",
    "\n",
    "        # Handle potential NaN prices more robustly\n",
    "        if pd.isna(price):\n",
    "            # Price is NaN: Cannot trade reliably. Treat as forced hold, zero reward.\n",
    "            # print(f\"Warning: NaN price encountered at step {self.current_step}. Forcing hold.\")\n",
    "            reward = 0.0\n",
    "            done = self.current_step >= len(self.data) - 2  # Check done condition again\n",
    "            self.current_step += 1  # Move step forward\n",
    "            # Portfolio value likely shouldn't change if price is NaN\n",
    "            # self.portfolio_value remains the same\n",
    "            return self._get_state(), reward, done\n",
    "        if pd.isna(next_price):\n",
    "            # Next price is NaN: Can execute trade at current price, but reward/next value is uncertain.\n",
    "            # Option: use current price for next value calculation (conservative)\n",
    "            # print(f\"Warning: NaN next_price encountered at step {self.current_step+1}. Using current price for value calculation.\")\n",
    "            next_price = price  # Fallback\n",
    "\n",
    "        initial_portfolio_value = self.portfolio_value\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            if (\n",
    "                self.position == 0 and self.balance > price\n",
    "            ):  # Check if enough balance for at least 1 share\n",
    "                shares_to_buy = self.balance // price\n",
    "                if shares_to_buy > 0:\n",
    "                    cost = shares_to_buy * price\n",
    "                    self.balance -= cost\n",
    "                    self.position = shares_to_buy\n",
    "            # If already holding or not enough balance, treat as hold\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            if self.position > 0:\n",
    "                revenue = self.position * price\n",
    "                self.balance += revenue\n",
    "                self.position = 0\n",
    "            # If not holding, treat as hold\n",
    "\n",
    "        # Update portfolio value using next_price (potentially the fallback price)\n",
    "        portfolio_value = self.balance + self.position * next_price\n",
    "\n",
    "        # Calculate returns based on the change from initial value for this step\n",
    "        if initial_portfolio_value > 1e-6:  # Avoid division by zero\n",
    "            returns = (\n",
    "                portfolio_value - initial_portfolio_value\n",
    "            ) / initial_portfolio_value\n",
    "        else:\n",
    "            returns = 0.0\n",
    "\n",
    "        self.returns_history.append(returns)\n",
    "        self.portfolio_value = (\n",
    "            portfolio_value  # Update portfolio value tracked by the env\n",
    "        )\n",
    "\n",
    "        # Advance step *before* getting the next state\n",
    "        self.current_step += 1\n",
    "\n",
    "        # CVaR reward adjustment\n",
    "        reward = returns\n",
    "        alpha = 0.05\n",
    "        min_history_for_cvar = 20\n",
    "        if len(self.returns_history) >= min_history_for_cvar:\n",
    "            cvar_penalty_factor = 0.1\n",
    "            calculated_cvar = self._calculate_cvar(\n",
    "                self.returns_history[-min_history_for_cvar:], alpha=alpha\n",
    "            )\n",
    "            reward = returns - cvar_penalty_factor * abs(calculated_cvar)\n",
    "\n",
    "        # Check if done (reached the end of data)\n",
    "        # Now done condition is simpler: if current_step points beyond the last valid index\n",
    "        done = (\n",
    "            self.current_step >= len(self.data) - 1\n",
    "        )  # -1 because step was already incremented\n",
    "\n",
    "        next_state = self._get_state()  # Get state for the *new* current_step\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def _calculate_cvar(self, returns, alpha=0.05):\n",
    "        if not isinstance(returns, np.ndarray):\n",
    "            returns = np.array(returns)\n",
    "        if len(returns) == 0:\n",
    "            return 0.0\n",
    "        var = np.percentile(returns, alpha * 100)\n",
    "        cvar = returns[returns <= var].mean()\n",
    "        return cvar if not np.isnan(cvar) else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming NoisyLinear class is defined as before:\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer(\"weight_epsilon\", torch.empty(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.empty(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / np.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / np.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(\n",
    "            size, device=self.weight_mu.device\n",
    "        )  # Ensure noise is on correct device\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure noise tensors are on the same device as parameters/input\n",
    "        if self.weight_epsilon.device != x.device:\n",
    "            self.weight_epsilon = self.weight_epsilon.to(x.device)\n",
    "            self.bias_epsilon = self.bias_epsilon.to(x.device)\n",
    "            # print(f\"Moved noise to {x.device} in NoisyLinear\") # Optional debug print\n",
    "\n",
    "        if self.training:\n",
    "            # Sample new noise only if training\n",
    "            self.reset_noise()\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            # Use mean weights/biases during evaluation\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        return nn.functional.linear(x, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        num_atoms=51,\n",
    "        v_min=-10,\n",
    "        v_max=10,\n",
    "        hidden_size=128,\n",
    "        device=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Rainbow DQN Network with Dueling Architecture.\n",
    "\n",
    "        Args:\n",
    "            state_size (int): Dimension of the state space.\n",
    "            action_size (int): Number of possible actions.\n",
    "            num_atoms (int): Number of atoms for the distributional RL value distribution.\n",
    "            v_min (float): Minimum value for the distribution support.\n",
    "            v_max (float): Maximum value for the distribution support.\n",
    "            hidden_size (int): Size of the hidden layers.\n",
    "            device (torch.device): Device to run the network on (CPU or CUDA).\n",
    "        \"\"\"\n",
    "        super(RainbowDQN, self).__init__()\n",
    "\n",
    "        # device setup\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Initializing RainbowDQN on device: {self.device}\")\n",
    "\n",
    "        # save sizes\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_atoms = num_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "\n",
    "        # support for C51 (distributional)\n",
    "        self.support = torch.linspace(v_min, v_max, num_atoms).to(self.device)\n",
    "\n",
    "        # --- Shared Feature Extraction Layers ---\n",
    "        # Using NoisyLinear for exploration baked into the network\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),  # First layer can be standard Linear\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(hidden_size, hidden_size),  # Subsequent layers are noisy\n",
    "            nn.ReLU(),\n",
    "        ).to(\n",
    "            self.device\n",
    "        )  # Ensure layers are moved to the correct device\n",
    "\n",
    "        # --- Dueling Architecture Streams ---\n",
    "        # 1. Value Stream: Estimates V(s) - output shape [batch_size, num_atoms]\n",
    "        self.value_stream = nn.Sequential(\n",
    "            NoisyLinear(hidden_size, hidden_size // 2),  # Smaller layer for value\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(hidden_size // 2, num_atoms),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # 2. Advantage Stream: Estimates A(s, a) - output shape [batch_size, action_size * num_atoms]\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            NoisyLinear(hidden_size, hidden_size // 2),  # Smaller layer for advantage\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(hidden_size // 2, action_size * num_atoms),\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Move the entire module to the specified device AFTER initializing layers\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the Dueling Rainbow DQN network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input state tensor, shape [batch_size, state_size].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output distribution over atoms for each action's Q-value,\n",
    "                          shape [batch_size, action_size, num_atoms].\n",
    "        \"\"\"\n",
    "        # Ensure input tensor is on the correct device\n",
    "        if x.device != self.device:\n",
    "            x = x.to(self.device)\n",
    "            # print(f\"Moved input tensor to {self.device} in forward pass\") # Optional debug print\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 1. Pass through shared feature layer\n",
    "        features = self.feature_layer(x)  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        # 2. Pass features through value and advantage streams\n",
    "        value_logits = self.value_stream(features)  # Shape: [batch_size, num_atoms]\n",
    "        advantage_logits = self.advantage_stream(\n",
    "            features\n",
    "        )  # Shape: [batch_size, action_size * num_atoms]\n",
    "\n",
    "        # 3. Reshape streams for combination\n",
    "        # Reshape value to be broadcastable: [batch_size, 1, num_atoms]\n",
    "        value_logits = value_logits.view(batch_size, 1, self.num_atoms)\n",
    "        # Reshape advantage: [batch_size, action_size, num_atoms]\n",
    "        advantage_logits = advantage_logits.view(\n",
    "            batch_size, self.action_size, self.num_atoms\n",
    "        )\n",
    "\n",
    "        # 4. Combine Value and Advantage streams (Dueling formula applied to logits)\n",
    "        # Q(s, a) = V(s) + (A(s, a) - mean(A(s, .)))\n",
    "        # Calculate mean advantage across actions for each atom\n",
    "        mean_advantage_logits = advantage_logits.mean(\n",
    "            dim=1, keepdim=True\n",
    "        )  # Shape: [batch_size, 1, num_atoms]\n",
    "\n",
    "        # Combine using broadcasting\n",
    "        q_logits = value_logits + (\n",
    "            advantage_logits - mean_advantage_logits\n",
    "        )  # Shape: [batch_size, action_size, num_atoms]\n",
    "\n",
    "        # 5. Apply Softmax to get the probability distribution over atoms for each action\n",
    "        # Softmax is applied along the last dimension (atoms)\n",
    "        dist = torch.softmax(\n",
    "            q_logits, dim=2\n",
    "        )  # Shape: [batch_size, action_size, num_atoms]\n",
    "\n",
    "        # Optional: Add a small epsilon to prevent log(0) issues later if needed,\n",
    "        # although the cross-entropy loss usually handles this.\n",
    "        # dist = dist.clamp(min=1e-8)\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Resets the noise in all NoisyLinear layers.\"\"\"\n",
    "        # Iterate all child modules recursively\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, NoisyLinear):\n",
    "                module.reset_noise()\n",
    "\n",
    "    def get_q_values(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the expected Q-values for each action from the output distribution.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): Input state tensor, shape [batch_size, state_size].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Expected Q-values for each action, shape [batch_size, action_size].\n",
    "        \"\"\"\n",
    "        # Ensure support is on the correct device\n",
    "        if self.support.device != self.device:\n",
    "            self.support = self.support.to(self.device)\n",
    "\n",
    "        dist = self.forward(state)  # Get the distribution [batch, action, atoms]\n",
    "        # Calculate expected value: sum(probability * support_value) for each action\n",
    "        q_values = (dist * self.support).sum(dim=2)  # Shape: [batch, action]\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4, n_step=3, gamma=0.99):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.buffer = []\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "\n",
    "    def _get_n_step_info(self):\n",
    "        \"\"\"Calculate multi-step return, next state, and done.\"\"\"\n",
    "        reward, next_state, done = self.n_step_buffer[-1][-3:]\n",
    "\n",
    "        for transition in reversed(list(self.n_step_buffer)[:-1]):\n",
    "            r, n_s, d = transition[-3:]\n",
    "            reward = r + self.gamma * reward * (1 - d)\n",
    "            if d:\n",
    "                next_state, done = n_s, d\n",
    "\n",
    "        state, action = self.n_step_buffer[0][:2]\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience to the n-step buffer and main buffer.\"\"\"\n",
    "        self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = self._get_n_step_info()\n",
    "\n",
    "        max_priority = self.priorities.max() if len(self.buffer) > 0 else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "\n",
    "        self.priorities[self.position] = max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == 0:\n",
    "            return None\n",
    "\n",
    "        probs = self.priorities[: len(self.buffer)] ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowDQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        # Hyperparameters (can be tuned)\n",
    "        v_min=-10.0,  # Adjust based on expected reward scale\n",
    "        v_max=10.0,  # Adjust based on expected reward scale\n",
    "        num_atoms=51,\n",
    "        hidden_size=128,\n",
    "        buffer_capacity=100000,\n",
    "        batch_size=32,\n",
    "        gamma=0.99,\n",
    "        n_step=3,\n",
    "        per_alpha=0.6,\n",
    "        per_beta=0.4,\n",
    "        target_update=1000,  # Often higher than 100, e.g., 1000 or 10000 steps\n",
    "        learning_rate=1e-4,  # Adam default is 1e-3, lower LR often better for RL\n",
    "    ):\n",
    "        self.device = torch.device(device)  # Ensure device is torch.device\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.n_step = n_step\n",
    "        self.target_update = target_update\n",
    "        self.steps_done = 0  # Track total optimization steps\n",
    "\n",
    "        # Distributional RL parameters\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.num_atoms = num_atoms\n",
    "        self.support = torch.linspace(v_min, v_max, num_atoms).to(self.device)\n",
    "        self.delta_z = (v_max - v_min) / (num_atoms - 1)\n",
    "\n",
    "        # Create networks with specified parameters\n",
    "        self.policy_net = RainbowDQN(\n",
    "            state_size, action_size, num_atoms, v_min, v_max, hidden_size, self.device\n",
    "        ).to(self.device)\n",
    "        self.target_net = RainbowDQN(\n",
    "            state_size, action_size, num_atoms, v_min, v_max, hidden_size, self.device\n",
    "        ).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Target network should be in evaluation mode\n",
    "\n",
    "        # Create optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Create replay buffer with N-step returns\n",
    "        self.memory = PrioritizedReplayBuffer(\n",
    "            buffer_capacity, per_alpha, per_beta, n_step, gamma\n",
    "        )\n",
    "\n",
    "        # Beta scheduling for PER (anneals beta from initial value to 1.0)\n",
    "        self.beta_start = per_beta\n",
    "        self.beta_frames = 100000  # Example: Anneal over 100k frames/steps\n",
    "        # Note: beta is annealed in the training loop, not here\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Selects action based on policy net using expected Q-values (NoisyNets handle exploration).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Ensure state is a float tensor and on the correct device\n",
    "            if not isinstance(state, torch.Tensor):\n",
    "                s = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "            else:\n",
    "                s = state.float().unsqueeze(0).to(self.device)\n",
    "\n",
    "            if s.shape[1] != self.policy_net.state_size:\n",
    "                raise ValueError(\n",
    "                    f\"State shape mismatch in select_action. Expected {self.policy_net.state_size}, got {s.shape[1]}\"\n",
    "                )\n",
    "\n",
    "            # Get expected Q-values using the helper function\n",
    "            q_values = self.policy_net.get_q_values(s)  # Shape: [1, action_size]\n",
    "            # Select action with the highest expected Q-value\n",
    "            return q_values.argmax(1).item()\n",
    "\n",
    "    def _categorical_projection(self, next_dist_target, rewards, dones, next_action):\n",
    "        \"\"\"\n",
    "        Performs the C51 categorical projection for the target distribution.\n",
    "        Operates on batches.\n",
    "\n",
    "        Args:\n",
    "            next_dist_target (Tensor): Target network's output distribution for next states. [B, A, N]\n",
    "            rewards (Tensor): Batch of rewards. [B]\n",
    "            dones (Tensor): Batch of done flags. [B] (True/False or 1/0)\n",
    "            next_action (Tensor): Batch of best actions in next state selected by target net. [B]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The projected target distribution for the chosen next actions. [B, N]\n",
    "        \"\"\"\n",
    "        batch_size = next_dist_target.size(0)\n",
    "        # Ensure inputs are on the correct device\n",
    "        rewards = rewards.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        next_action = next_action.to(self.device)\n",
    "\n",
    "        # Get the distributions corresponding to the best next actions\n",
    "        # Gather requires index shape to match input shape except along the gathering dimension\n",
    "        next_action = next_action.view(batch_size, 1, 1).expand(\n",
    "            -1, -1, self.num_atoms\n",
    "        )  # [B, 1, N]\n",
    "        next_dist_best_action = next_dist_target.gather(1, next_action).squeeze(\n",
    "            1\n",
    "        )  # [B, N]\n",
    "\n",
    "        # Mask dones for calculations: 1.0 if not done, 0.0 if done\n",
    "        dones_mask = (~dones).float().unsqueeze(1)  # Shape [B, 1]\n",
    "\n",
    "        # Calculate the projected support atoms Tz = R + gamma^N * z (where N is n_step)\n",
    "        # Rewards are already N-step adjusted by the buffer\n",
    "        Tz = (\n",
    "            rewards.unsqueeze(1)\n",
    "            + (self.gamma**self.n_step) * self.support.unsqueeze(0) * dones_mask\n",
    "        )  # [B, N]\n",
    "\n",
    "        # Clip projected atoms to [Vmin, Vmax]\n",
    "        Tz = Tz.clamp(self.v_min, self.v_max)\n",
    "\n",
    "        # Compute bin indices and offsets\n",
    "        b = (Tz - self.v_min) / self.delta_z  # [B, N]\n",
    "        lower_bound = b.floor().long()  # [B, N]\n",
    "        upper_bound = b.ceil().long()  # [B, N]\n",
    "\n",
    "        # Handle boundary cases where b is exactly an integer (l == u)\n",
    "        # Create masks for these cases\n",
    "        lower_eq_mask = (\n",
    "            lower_bound == b\n",
    "        )  # Where projection falls exactly on lower bin edge\n",
    "        upper_eq_mask = (\n",
    "            upper_bound == b\n",
    "        )  # Where projection falls exactly on upper bin edge (redundant if l==u?)\n",
    "\n",
    "        # Initialize target distribution tensor\n",
    "        target_dist = torch.zeros(\n",
    "            batch_size, self.num_atoms, device=self.device\n",
    "        )  # [B, N]\n",
    "\n",
    "        # Calculate weights for lower and upper bins (distribution factors)\n",
    "        # Ensure float division/subtraction before multiplying probability\n",
    "        weight_l = (upper_bound.float() - b) * next_dist_best_action  # [B, N]\n",
    "        weight_u = (b - lower_bound.float()) * next_dist_best_action  # [B, N]\n",
    "\n",
    "        # --- Distribute probability mass using scatter_add_ ---\n",
    "        # Add weights to the lower bin indices\n",
    "        # Prevent index out of bounds for lower_bound == num_atoms (when b is exactly Vmax)\n",
    "        lower_bound = lower_bound.clamp(max=self.num_atoms - 1)\n",
    "        target_dist.scatter_add_(dim=1, index=lower_bound, src=weight_l)\n",
    "\n",
    "        # Add weights to the upper bin indices\n",
    "        # Prevent index out of bounds for upper_bound == 0 (when b is exactly Vmin)\n",
    "        # Note: scatter_add_ handles indices >= target.shape[dim] by ignoring them, but clamping is safer.\n",
    "        upper_bound = upper_bound.clamp(min=0)\n",
    "        target_dist.scatter_add_(dim=1, index=upper_bound, src=weight_u)\n",
    "\n",
    "        # The above scatter_add handles the l==u case automatically if done carefully.\n",
    "        # When l == u, b is an integer.\n",
    "        # weight_l = (l - b) * p = 0 * p = 0\n",
    "        # weight_u = (b - l) * p = 0 * p = 0\n",
    "        # This seems wrong. Let's rethink the l==u case distribution.\n",
    "\n",
    "        # --- Corrected Distribution Logic for l == u ---\n",
    "        # If l == u, it means b is an integer, and the entire probability p_j\n",
    "        # should be assigned *only* to index l (or u).\n",
    "        # The scatter_add approach might be tricky here. Let's use the loop for clarity,\n",
    "        # then consider vectorization if needed.\n",
    "\n",
    "        # --- Reverting to Loop for Clarity (can be slow) ---\n",
    "        target_dist_loop = torch.zeros(\n",
    "            batch_size, self.num_atoms, device=self.device\n",
    "        )  # [B, N]\n",
    "        for i in range(batch_size):\n",
    "            if dones[i]:\n",
    "                # If done, target is a Dirac delta at the (clipped) reward\n",
    "                Tz_done = rewards[i].clamp(self.v_min, self.v_max)\n",
    "                b_done = (Tz_done - self.v_min) / self.delta_z\n",
    "                l_done = b_done.floor().long()\n",
    "                u_done = b_done.ceil().long()\n",
    "                if l_done == u_done:\n",
    "                    target_dist_loop[i, l_done] = 1.0\n",
    "                else:\n",
    "                    target_dist_loop[i, l_done] = u_done.float() - b_done\n",
    "                    target_dist_loop[i, u_done] = b_done - l_done.float()\n",
    "            else:\n",
    "                # If not done, project the next state distribution atom by atom\n",
    "                for j in range(\n",
    "                    self.num_atoms\n",
    "                ):  # Index of atom in next state distribution\n",
    "                    p_j = next_dist_best_action[i, j]  # Probability of this atom\n",
    "                    if p_j > 1e-8:  # Optimization: skip if probability is negligible\n",
    "                        Tz_j = Tz[i, j]  # Pre-calculated clipped projected atom value\n",
    "                        b_j = b[i, j]  # Pre-calculated bin position\n",
    "                        l_j = lower_bound[i, j]  # Index of lower bin\n",
    "                        u_j = upper_bound[i, j]  # Index of upper bin\n",
    "\n",
    "                        # Distribute probability p_j to bins l_j and u_j\n",
    "                        if l_j == u_j:\n",
    "                            target_dist_loop[i, l_j] += p_j\n",
    "                        else:\n",
    "                            target_dist_loop[i, l_j] += p_j * (u_j.float() - b_j)\n",
    "                            target_dist_loop[i, u_j] += p_j * (b_j - l_j.float())\n",
    "\n",
    "        # Normalize distribution? Usually not needed if projection is correct.\n",
    "        # sum_check = target_dist_loop.sum(dim=1)\n",
    "        # if not torch.allclose(sum_check, torch.ones_like(sum_check)):\n",
    "        #      print(\"Warning: Target distribution sum is not 1. Sums:\", sum_check)\n",
    "\n",
    "        return target_dist_loop  # Return the loop-based version for correctness\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \"\"\"Samples batch, computes loss, and updates policy network.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None  # Not enough samples yet\n",
    "\n",
    "        # Anneal beta for PER importance sampling\n",
    "        beta = min(\n",
    "            1.0,\n",
    "            self.beta_start\n",
    "            + self.steps_done * (1.0 - self.beta_start) / self.beta_frames,\n",
    "        )\n",
    "\n",
    "        # Sample from replay buffer\n",
    "        # Need to pass current beta to the sample method if it uses it directly\n",
    "        # Assuming the buffer's sample method uses its internal self.beta and we update it here (if needed)\n",
    "        # self.memory.beta = beta # Update beta in buffer if it's used there dynamically\n",
    "        transitions, indices, weights = self.memory.sample(\n",
    "            self.batch_size\n",
    "        )  # Weights are calculated using buffer's beta\n",
    "        batch = list(zip(*transitions))\n",
    "\n",
    "        # Convert batch data to tensors\n",
    "        states_np = np.array(batch[0], dtype=np.float32)\n",
    "        actions_np = np.array(batch[1], dtype=np.int64)\n",
    "        rewards_np = np.array(batch[2], dtype=np.float32)  # These are N-step rewards\n",
    "        next_states_np = np.array(batch[3], dtype=np.float32)  # This is state S_{t+N}\n",
    "        dones_np = np.array(batch[4], dtype=bool)  # Done flags for S_{t+N}\n",
    "\n",
    "        state_batch = torch.from_numpy(states_np).to(self.device)\n",
    "        action_batch = torch.from_numpy(actions_np).to(self.device)  # [B]\n",
    "        reward_batch = torch.from_numpy(rewards_np).to(self.device)  # [B]\n",
    "        next_state_batch = torch.from_numpy(next_states_np).to(self.device)\n",
    "        done_batch = torch.from_numpy(dones_np).to(self.device)  # [B]\n",
    "        weights = (\n",
    "            torch.from_numpy(np.array(weights, dtype=np.float32))\n",
    "            .to(self.device)\n",
    "            .unsqueeze(1)\n",
    "        )  # [B, 1] for broadcasting loss\n",
    "\n",
    "        # --- Target Calculation ---\n",
    "        with torch.no_grad():\n",
    "            # Get next state distributions and expected Q-values from TARGET network\n",
    "            next_dist_target = self.target_net(next_state_batch)  # [B, A, N]\n",
    "            next_q_target = (next_dist_target * self.support).sum(dim=2)  # [B, A]\n",
    "\n",
    "            # Select best next action based on TARGET network's expected Q-values\n",
    "            next_action = next_q_target.argmax(dim=1)  # [B]\n",
    "\n",
    "            # Project the target distribution for the selected next actions\n",
    "            target_dist = self._categorical_projection(\n",
    "                next_dist_target,  # Distribution from target net [B, A, N]\n",
    "                reward_batch,  # N-step Rewards [B]\n",
    "                done_batch,  # Done flags for S_{t+N} [B]\n",
    "                next_action,  # Best action in S_{t+N} selected by target net [B]\n",
    "            )  # Result shape: [B, N]\n",
    "\n",
    "        # --- Loss Calculation ---\n",
    "        # Get current state distributions from POLICY network\n",
    "        current_dist = self.policy_net(state_batch)  # [B, A, N]\n",
    "\n",
    "        # Get the distribution for the action actually taken (action_batch)\n",
    "        # Need to gather based on action_batch index\n",
    "        action_batch_expanded = action_batch.view(self.batch_size, 1, 1).expand(\n",
    "            -1, -1, self.num_atoms\n",
    "        )  # [B, 1, N]\n",
    "        current_dist_taken_action = current_dist.gather(\n",
    "            1, action_batch_expanded\n",
    "        ).squeeze(\n",
    "            1\n",
    "        )  # [B, N]\n",
    "\n",
    "        # Compute cross-entropy loss between target and current distributions\n",
    "        # Add small epsilon for numerical stability before log\n",
    "        loss = -(target_dist * torch.log(current_dist_taken_action + 1e-8)).sum(\n",
    "            dim=1\n",
    "        )  # [B]\n",
    "\n",
    "        # Apply PER weights: loss = loss * weights\n",
    "        # Loss shape is [B], weights shape is [B, 1], squeeze weights or unsqueeze loss\n",
    "        weighted_loss = (\n",
    "            loss * weights.squeeze(1)\n",
    "        ).mean()  # Calculate the mean weighted loss\n",
    "\n",
    "        # --- Update Priorities in PER Buffer ---\n",
    "        # Priorities are typically based on the absolute loss (or TD error)\n",
    "        # Add a small epsilon before taking power alpha for stability\n",
    "        new_priorities = loss.abs().detach().cpu().numpy() + 1e-6\n",
    "        self.memory.update_priorities(indices, new_priorities)\n",
    "\n",
    "        # --- Optimize Policy Network ---\n",
    "        self.optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "        # Clip gradients to prevent explosions\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # --- Reset Noise ---\n",
    "        # Reset noise in NoisyLinear layers (for both networks if target uses them)\n",
    "        # Important for exploration when using Noisy Nets\n",
    "        self.policy_net.reset_noise()\n",
    "        self.target_net.reset_noise()\n",
    "\n",
    "        # --- Update Target Network --- (Soft update often more stable, but periodic hard update is simpler)\n",
    "        self.steps_done += 1  # Increment optimization step counter\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            print(f\"--- Updating target network at step {self.steps_done} ---\")\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        return weighted_loss.item()  # Return loss value for logging if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example modification in train_agent loop:\n",
    "def train_agent(\n",
    "    env, agent, num_episodes=1000, max_steps_per_episode=10000\n",
    "):  # Add max steps\n",
    "    returns_history = []\n",
    "    losses_history = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_return = 0\n",
    "        episode_losses = []\n",
    "        done = False\n",
    "        steps_in_episode = 0  # Track steps within episode\n",
    "\n",
    "        while not done and steps_in_episode < max_steps_per_episode:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "\n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Store transition in memory (using N-step buffer)\n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "\n",
    "            # Optimize model (only if buffer is large enough)\n",
    "            loss_val = agent.optimize_model()  # optimize_model increments steps_done\n",
    "            if loss_val is not None:\n",
    "                episode_losses.append(loss_val)\n",
    "\n",
    "            steps_in_episode += 1\n",
    "\n",
    "        returns_history.append(\n",
    "            env.portfolio_value\n",
    "        )  # Store final portfolio value or total return\n",
    "        avg_loss = np.mean(episode_losses) if episode_losses else 0\n",
    "        losses_history.append(avg_loss)\n",
    "        print(\n",
    "            f\"Episode {episode + 1}/{num_episodes}, \"\n",
    "            f\"Steps: {steps_in_episode}, \"\n",
    "            f\"Total Steps: {agent.steps_done}, \"\n",
    "            f\"Return: {env.portfolio_value:.2f}, \"  # Print final portfolio value\n",
    "            f\"Avg Loss: {avg_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    return returns_history, losses_history, agent  # Return losses too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Collect Data\n",
    "stock_data = collect_stock_data()\n",
    "processed_data = calculate_technical_indicators(stock_data.copy())\n",
    "processed_data.dropna(inplace=True)\n",
    "\n",
    "# EDA Output\n",
    "print(\"Dataset shape:\", processed_data.shape)\n",
    "print(\"  Date range:\", processed_data.index.min(), \"to\", processed_data.index.max())\n",
    "# print(processed_data.head()) # Keep short\n",
    "\n",
    "# Create environment and agent\n",
    "env = TradingEnvironment(processed_data, initial_balance=100000)  # Example balance\n",
    "state_size = env.state_size\n",
    "action_size = 3  # Buy, Hold, Sell\n",
    "\n",
    "# Use a specific device\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device_name}\")\n",
    "\n",
    "agent = RainbowDQNAgent(\n",
    "    state_size, action_size, device=device_name\n",
    ")  # Pass explicit device\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "num_train_episodes = 5  # Adjust number of episodes\n",
    "print(f\"Starting training for {num_train_episodes} episodes...\")\n",
    "returns_history, losses_history, agent = train_agent(\n",
    "    env, agent, num_episodes=num_train_episodes\n",
    ")\n",
    "\n",
    "# Plot returns (final portfolio value) and losses\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "color = \"tab:red\"\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Final Portfolio Value\", color=color)\n",
    "ax1.plot(returns_history, color=color, label=\"Portfolio Value\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "ax1.axhline(\n",
    "    y=env.initial_balance, color=\"gray\", linestyle=\"--\", label=\"Initial Balance\"\n",
    ")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = \"tab:blue\"\n",
    "ax2.set_ylabel(\"Average Loss\", color=color)\n",
    "ax2.plot(losses_history, color=color, alpha=0.6, label=\"Avg Loss\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.title(\"Training Performance: Portfolio Value and Average Loss per Episode\")\n",
    "plt.savefig(\n",
    "    \"/home/kartikeya.agrawal_ug25/RL_Final/plot.png\", dpi=300, transparent=True\n",
    ")\n",
    "\n",
    "# --- Checkpoint Saving ---\n",
    "save_path = \"/home/kartikeya.agrawal_ug25/RL_Final/rainbow_dqn_agent_checkpoint_final.pth\"  # Example path\n",
    "try:\n",
    "    torch.save(\n",
    "        {\n",
    "            \"policy_net_state_dict\": agent.policy_net.state_dict(),\n",
    "            \"target_net_state_dict\": agent.target_net.state_dict(),\n",
    "            \"optimizer_state_dict\": agent.optimizer.state_dict(),\n",
    "            \"steps_done\": agent.steps_done,\n",
    "            # Optional: Save Vmin/Vmax/NumAtoms if they might change\n",
    "            \"v_min\": agent.v_min,\n",
    "            \"v_max\": agent.v_max,\n",
    "            \"num_atoms\": agent.num_atoms,\n",
    "            # Optional: Save replay buffer (can be large)\n",
    "            # \"replay_buffer\": agent.memory\n",
    "        },\n",
    "        save_path,\n",
    "    )\n",
    "    print(f\"Checkpoint saved successfully to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving checkpoint: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
